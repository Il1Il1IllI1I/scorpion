{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\성현태\\AppData\\Local\\Temp\\ipykernel_20300\\2348841028.py:33: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.median(level=1) should use df.groupby(level=1).median().\n",
      "  top10_avg = kosdaq_companies_data.groupby('회계년도')[col].nlargest(10).mean(level=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\2348841028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m# SMOTE 적용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0msmote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mX_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m# 데이터 분할\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         )\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1147\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    958\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             )\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터셋 B (코스닥 전체 기업) 로드\n",
    "kosdaq_companies_file_path = '111.csv'\n",
    "kosdaq_companies_data = pd.read_csv(kosdaq_companies_file_path, encoding='CP949')\n",
    "\n",
    "# 데이터셋 A (상장폐지 기업) 로드 및 전처리\n",
    "kosdaq_delisted_file_path = '상장폐지기업사유(피흡수합병제외).xlsx'\n",
    "kosdaq_delisted_data = pd.read_excel(kosdaq_delisted_file_path)\n",
    "\n",
    "# 데이터셋 A의 결측치 제거 및 이상치 처리\n",
    "numeric_columns = kosdaq_delisted_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_columns:\n",
    "    kosdaq_delisted_data[col] = winsorize(kosdaq_delisted_data[col], limits=[0.01, 0.01])  # 이상치 처리\n",
    "\n",
    "# 데이터셋 B의 재무비율 칼럼 식별 및 회계년도 형식 변환\n",
    "financial_ratios_columns = kosdaq_companies_data.columns.drop(['회사명', '거래소코드', '회계년도'])\n",
    "kosdaq_companies_data['회계년도'] = kosdaq_companies_data['회계년도'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# 각 재무비율에 대해 파생변수 생성\n",
    "for col in financial_ratios_columns:\n",
    "    # 시차 차이 계산\n",
    "    kosdaq_companies_data[f'{col}_시차차이'] = kosdaq_companies_data.groupby('회사명')[col].diff()\n",
    "    # 시차 비율 계산\n",
    "    kosdaq_companies_data[f'{col}_시차비율'] = kosdaq_companies_data.groupby('회사명')[col].pct_change()\n",
    "    # 연도별 상위 10개 기업 평균과의 차이\n",
    "    top10_avg = kosdaq_companies_data.groupby('회계년도')[col].nlargest(10).mean(level=0)\n",
    "    kosdaq_companies_data = kosdaq_companies_data.join(top10_avg, on='회계년도', rsuffix='_상위10평균')\n",
    "    kosdaq_companies_data[f'{col}_상위10차이'] = kosdaq_companies_data[f'{col}_상위10평균'] - kosdaq_companies_data[col]\n",
    "    # 연도별 전체 평균과의 차이\n",
    "    overall_avg = kosdaq_companies_data.groupby('회계년도')[col].transform('mean')\n",
    "    kosdaq_companies_data[f'{col}_전체평균차이'] = overall_avg - kosdaq_companies_data[col]\n",
    "\n",
    "# 상위 10개 기업 평균 관련 열 제거\n",
    "top10_avg_columns = [col for col in kosdaq_companies_data.columns if '_상위10평균' in col]\n",
    "kosdaq_companies_data_cleaned = kosdaq_companies_data.drop(columns=top10_avg_columns)\n",
    "\n",
    "# 데이터셋 A와 B를 레이블링\n",
    "kosdaq_delisted_data_cleaned = kosdaq_delisted_data.copy()\n",
    "kosdaq_delisted_data_cleaned['부도여부'] = 1\n",
    "kosdaq_companies_data_cleaned['부도여부'] = 0\n",
    "\n",
    "# 데이터 병합\n",
    "combined_data = pd.concat([kosdaq_companies_data_cleaned, kosdaq_delisted_data_cleaned], ignore_index=True)\n",
    "\n",
    "financial_ratios_columns = ['매출액증가율(IFRS)', '총자본증가율(IFRS)', '매출액순이익률(IFRS)', '총자본정상영업이익률(IFRS)',\n",
    "                            'CASH FLOW 대 부채비율(IFRS)', 'CASH FLOW 대 총자본비율(IFRS)', '총자본회전률(IFRS)',\n",
    "                            '자기자본회전률(IFRS)', '총자본투자효율(IFRS)', '설비투자효율(IFRS)']\n",
    "\n",
    "X = kosdaq_companies_data_cleaned[financial_ratios_columns]\n",
    "\n",
    "# 결측치를 0으로 대체\n",
    "# X = X.fillna(0)\n",
    "\n",
    "y = combined_data['부도여부']\n",
    "\n",
    "# SMOTE 적용\n",
    "smote = SMOTE(random_state=0)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=0)\n",
    "\n",
    "# RandomForest 모델 생성 및 학습\n",
    "rf_classifier = RandomForestClassifier(random_state=0)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 모델 성능 평가\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 특성 중요도 분석 및 시각화\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "features = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "sorted_idx = feature_importance_df['Importance'].argsort()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(feature_importances)), feature_importance_df['Importance'][sorted_idx], align='center')\n",
    "plt.yticks(range(len(feature_importances)), [features[i] for i in sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "# 모델 성능 보고서 출력\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\성현태\\AppData\\Local\\Temp\\ipykernel_20300\\2507917726.py:33: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.median(level=1) should use df.groupby(level=1).median().\n",
      "  top10_avg = kosdaq_companies_data.groupby('회계년도')[col].nlargest(10).mean(level=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The target 'y' needs to have more than 1 class. Got 1 class instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20300\\2507917726.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m# SMOTE 적용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0msmote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minority'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 'minority' 클래스 쪽만 조정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mX_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m# 데이터 분할\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    517\u001b[0m             \u001b[1;34mf\"The target 'y' needs to have more than 1 class. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[1;34mf\"Got {np.unique(y).size} class instead\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The target 'y' needs to have more than 1 class. Got 1 class instead"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터셋 B (코스닥 전체 기업) 로드\n",
    "kosdaq_companies_file_path = '111.csv'\n",
    "kosdaq_companies_data = pd.read_csv(kosdaq_companies_file_path, encoding='CP949')\n",
    "\n",
    "# 데이터셋 A (상장폐지 기업) 로드 및 전처리\n",
    "kosdaq_delisted_file_path = '상장폐지기업사유(피흡수합병제외).xlsx'\n",
    "kosdaq_delisted_data = pd.read_excel(kosdaq_delisted_file_path)\n",
    "\n",
    "# 데이터셋 A의 결측치 제거 및 이상치 처리\n",
    "numeric_columns = kosdaq_delisted_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_columns:\n",
    "    kosdaq_delisted_data[col] = winsorize(kosdaq_delisted_data[col], limits=[0.01, 0.01])  # 이상치 처리\n",
    "\n",
    "# 데이터셋 B의 재무비율 칼럼 식별 및 회계년도 형식 변환\n",
    "financial_ratios_columns = kosdaq_companies_data.columns.drop(['회사명', '거래소코드', '회계년도'])\n",
    "kosdaq_companies_data['회계년도'] = kosdaq_companies_data['회계년도'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# 각 재무비율에 대해 파생변수 생성\n",
    "for col in financial_ratios_columns:\n",
    "    # 시차 차이 계산\n",
    "    kosdaq_companies_data[f'{col}_시차차이'] = kosdaq_companies_data.groupby('회사명')[col].diff()\n",
    "    # 시차 비율 계산\n",
    "    kosdaq_companies_data[f'{col}_시차비율'] = kosdaq_companies_data.groupby('회사명')[col].pct_change()\n",
    "    # 연도별 상위 10개 기업 평균과의 차이\n",
    "    top10_avg = kosdaq_companies_data.groupby('회계년도')[col].nlargest(10).mean(level=0)\n",
    "    kosdaq_companies_data = kosdaq_companies_data.join(top10_avg, on='회계년도', rsuffix='_상위10평균')\n",
    "    kosdaq_companies_data[f'{col}_상위10차이'] = kosdaq_companies_data[f'{col}_상위10평균'] - kosdaq_companies_data[col]\n",
    "    # 연도별 전체 평균과의 차이\n",
    "    overall_avg = kosdaq_companies_data.groupby('회계년도')[col].transform('mean')\n",
    "    kosdaq_companies_data[f'{col}_전체평균차이'] = overall_avg - kosdaq_companies_data[col]\n",
    "\n",
    "# 상위 10개 기업 평균 관련 열 제거\n",
    "top10_avg_columns = [col for col in kosdaq_companies_data.columns if '_상위10평균' in col]\n",
    "kosdaq_companies_data_cleaned = kosdaq_companies_data.drop(columns=top10_avg_columns)\n",
    "\n",
    "# 데이터셋 A와 B를 레이블링\n",
    "kosdaq_delisted_data_cleaned = kosdaq_delisted_data.copy()\n",
    "kosdaq_delisted_data_cleaned['부도여부'] = 1\n",
    "kosdaq_companies_data_cleaned['부도여부'] = 0\n",
    "\n",
    "# 데이터 병합\n",
    "combined_data = pd.concat([kosdaq_companies_data_cleaned, kosdaq_delisted_data_cleaned], ignore_index=True)\n",
    "\n",
    "financial_ratios_columns = ['매출액증가율(IFRS)', '총자본증가율(IFRS)', '매출액순이익률(IFRS)', '총자본정상영업이익률(IFRS)',\n",
    "                            'CASH FLOW 대 부채비율(IFRS)', 'CASH FLOW 대 총자본비율(IFRS)', '총자본회전률(IFRS)',\n",
    "                            '자기자본회전률(IFRS)', '총자본투자효율(IFRS)', '설비투자효율(IFRS)']\n",
    "\n",
    "X = kosdaq_companies_data_cleaned[financial_ratios_columns]\n",
    "\n",
    "# 결측치를 0으로 대체\n",
    "X = X.fillna(0)\n",
    "\n",
    "y = combined_data['부도여부']\n",
    "\n",
    "# y의 개수를 일치시키기 위해 자르기\n",
    "y = y[:len(X)]\n",
    "\n",
    "# SMOTE 적용\n",
    "smote = SMOTE(random_state=0, sampling_strategy='minority')  # 'minority' 클래스 쪽만 조정\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=0)\n",
    "\n",
    "# RandomForest 모델 생성 및 학습\n",
    "rf_classifier = RandomForestClassifier(random_state=0)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 모델 성능 평가\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 특성 중요도 분석 및 시각화\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "features = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "sorted_idx = feature_importance_df['Importance'].argsort()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(feature_importances)), feature_importance_df['Importance'][sorted_idx], align='center')\n",
    "plt.yticks(range(len(feature_importances)), [features[i] for i in sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "# 모델 성능 보고서 출력\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['매출액증가율(IFRS)', '총자본증가율(IFRS)', '매출액순이익률(IFRS)', '총자본정상영업이익률(IFRS)',\n",
       "       'CASH FLOW 대 부채비율(IFRS)', 'CASH FLOW 대 총자본비율(IFRS)', '총자본회전률(IFRS)',\n",
       "       '자기자본회전률(IFRS)', '총자본투자효율(IFRS)', '설비투자효율(IFRS)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
